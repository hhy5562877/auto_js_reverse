from __future__ import annotations

import asyncio
import json
import logging
import re
from pathlib import Path
from typing import Optional

from fastmcp import FastMCP

from .services.pipeline import Pipeline

logger = logging.getLogger(__name__)

BASE_DIR = Path(__file__).resolve().parent.parent.parent
CONFIG_PATH = BASE_DIR / ".mcp_config" / "config.json"


def _load_config() -> dict:
    if CONFIG_PATH.exists():
        return json.loads(CONFIG_PATH.read_text(encoding="utf-8"))
    return {}


config = _load_config()
pipeline = Pipeline(config=config, base_dir=BASE_DIR)

mcp = FastMCP(name="Browser Insight MCP")


@mcp.tool
async def capture_current_page(
    storage_path: str, target_url: Optional[str] = None, force_refresh: bool = False
) -> str:
    """è§¦å‘ä¸€æ¬¡å®Œæ•´çš„å…¨é‡æŠ“å–ã€å½’æ¡£ã€åˆ†æžæµç¨‹ã€‚
    æŠ“å– Chrome é¡µé¢çš„æ‰€æœ‰ JS èµ„æºï¼Œè¿›è¡Œ Source Map è¿˜åŽŸã€AST è¯­ä¹‰åˆ‡åˆ†ã€å‘é‡åŒ–ç´¢å¼•ã€‚
    æ‰€æœ‰åŽŸå§‹ JS æ–‡ä»¶ã€Source Map å’Œè¿˜åŽŸåŽçš„æºç éƒ½ä¼šä¿å­˜åˆ°æŒ‡å®šçš„å­˜å‚¨è·¯å¾„ä¸‹ã€‚

    å¦‚æžœç”¨æˆ·å·²æœ‰æµè§ˆå™¨æ‰“å¼€äº†ç›®æ ‡ç½‘é¡µï¼Œä¼šè‡ªåŠ¨å¤ç”¨è¯¥æ ‡ç­¾é¡µï¼›å¦‚æžœæ²¡æœ‰åˆ™æ–°å»ºæ ‡ç­¾é¡µå¹¶å¯¼èˆªã€‚
    ä¸æŒ‡å®š target_url æ—¶ï¼Œä½¿ç”¨å½“å‰æ´»è·ƒæ ‡ç­¾é¡µã€‚

    Args:
        storage_path: æ–‡ä»¶å­˜å‚¨çš„ç»å¯¹è·¯å¾„ï¼Œæ‰€æœ‰æŠ“å–çš„ JS æ–‡ä»¶å°†å½’æ¡£åˆ°æ­¤ç›®å½•ä¸‹ï¼ˆæŒ‰æ—¥æœŸ/åŸŸå/åŽŸå§‹è·¯å¾„ç»„ç»‡ï¼‰
        target_url: ç›®æ ‡ç½‘é¡µ URLï¼Œä¾‹å¦‚ "https://www.baidu.com"ã€‚ä¼šè‡ªåŠ¨æŸ¥æ‰¾å·²æ‰“å¼€çš„åŒ¹é…æ ‡ç­¾é¡µï¼Œæ‰¾ä¸åˆ°åˆ™æ–°å»º
        force_refresh: æ˜¯å¦å¿½ç•¥å“ˆå¸Œç¼“å­˜ï¼Œå¼ºåˆ¶é‡æ–°è§£æžæ‰€æœ‰æ–‡ä»¶
    """
    try:
        stats = await pipeline.capture_page(
            force_refresh=force_refresh,
            storage_path=storage_path,
            target_url=target_url,
        )
    except ConnectionRefusedError as e:
        return f"âŒ è¿žæŽ¥å¤±è´¥: {e}"
    except Exception as e:
        logger.exception("capture_current_page å¼‚å¸¸")
        return f"âŒ æŠ“å–å¤±è´¥: {e}"

    parts = []
    parts.append("âœ… æŠ“å–å®Œæˆ")
    parts.append(f"æ–°å¢ž {stats['new_files']} ä¸ª JS æ–‡ä»¶")
    if stats["skipped"] > 0:
        parts.append(f"è·³è¿‡ {stats['skipped']} ä¸ªå·²ç´¢å¼•æ–‡ä»¶")
    if stats["source_maps"] > 0:
        parts.append(f"è¿˜åŽŸäº† {stats['source_maps']} ä¸ª Source Map")
    parts.append(f"å…±ç´¢å¼• {stats['chunks_indexed']} ä¸ªä»£ç å—")
    parts.append(f"å­˜å‚¨è·¯å¾„: {stats['storage_path']}")
    return "ï¼Œ".join(parts) + "ã€‚"


@mcp.tool
async def search_local_codebase(
    query: str, domain_filter: Optional[str] = None, limit: int = 10
) -> str:
    """RAG è¯­ä¹‰æ£€ç´¢æœ¬åœ°å·²ç´¢å¼•çš„æµè§ˆå™¨ JS ä»£ç åº“ã€‚
    ç”¨è‡ªç„¶è¯­è¨€æè¿°ä½ è¦æ‰¾çš„åŠŸèƒ½ï¼Œè¿”å›žæœ€ç›¸å…³çš„ä»£ç ç‰‡æ®µã€‚

    Args:
        query: è‡ªç„¶è¯­è¨€æœç´¢é—®é¢˜ï¼Œä¾‹å¦‚ "ç”¨æˆ·ç™»å½•é€»è¾‘"ã€"API è¯·æ±‚ç­¾å"ã€"åŠ å¯†å‡½æ•°"
        domain_filter: é™åˆ¶æœç´¢çš„åŸŸåï¼Œä¾‹å¦‚ "example.com"
        limit: è¿”å›žç»“æžœæ•°é‡ä¸Šé™
    """
    try:
        results = await pipeline.search(
            query=query, domain_filter=domain_filter, limit=limit
        )
    except Exception as e:
        logger.exception("search_local_codebase å¼‚å¸¸")
        return f"âŒ æœç´¢å¤±è´¥: {e}"

    if not results:
        return "æœªæ‰¾åˆ°ç›¸å…³ä»£ç ã€‚è¯·å…ˆä½¿ç”¨ capture_current_page æŠ“å–é¡µé¢ã€‚"

    seen_texts: set[str] = set()
    unique_results = []
    for r in results:
        text_key = r.get("text", "")[:200]
        if text_key not in seen_texts:
            seen_texts.add(text_key)
            unique_results.append(r)

    output_parts = []
    for i, r in enumerate(unique_results, 1):
        source_tag = (
            "ðŸ”„ Source Map è¿˜åŽŸ" if r.get("source_map_restored") else "ðŸ“¦ æ··æ·†ä»£ç "
        )
        header = (
            f"### ç»“æžœ {i} [{source_tag}]\n"
            f"- æ–‡ä»¶: `{r.get('original_file', 'unknown')}`\n"
            f"- æ¥æº: `{r.get('url', '')}`\n"
            f"- è¡Œå·: {r.get('line_start', '?')}-{r.get('line_end', '?')}\n"
        )
        code = f"```javascript\n{r.get('text', '')}\n```"
        output_parts.append(header + code)

    return "\n\n".join(output_parts)


@mcp.tool
async def list_captured_files(domain_filter: Optional[str] = None) -> str:
    """åˆ—å‡ºæœ¬åœ°å·²æŠ“å–å½’æ¡£çš„æ‰€æœ‰ JS æ–‡ä»¶ã€‚
    å¯æŒ‰åŸŸåè¿‡æ»¤ã€‚è¿”å›žæ¯ä¸ªæ–‡ä»¶çš„ URLã€æœ¬åœ°è·¯å¾„ã€æ˜¯å¦æœ‰ Source Mapã€‚
    ç”¨äºŽäº†è§£å½“å‰å·²æŠ“å–äº†å“ªäº›èµ„æºï¼Œå†å†³å®šç”¨ read_js_file è¯»å–å…·ä½“æ–‡ä»¶ã€‚

    Args:
        domain_filter: é™åˆ¶åˆ—å‡ºçš„åŸŸåï¼Œä¾‹å¦‚ "www.baidu.com"ã€‚ä¸å¡«åˆ™åˆ—å‡ºæ‰€æœ‰åŸŸå
    """
    files = pipeline.index.list_files_by_domain(domain=domain_filter)
    if not files:
        hint = f" (åŸŸå: {domain_filter})" if domain_filter else ""
        return f"æš‚æ— å·²æŠ“å–çš„æ–‡ä»¶{hint}ã€‚è¯·å…ˆä½¿ç”¨ capture_current_page æŠ“å–é¡µé¢ã€‚"

    lines = [f"ðŸ“ å·²æŠ“å–æ–‡ä»¶åˆ—è¡¨ (å…± {len(files)} ä¸ª)\n"]
    for f in files:
        sm = "âœ… æœ‰ Source Map" if f.get("source_map_restored") else "âŒ æ—  Source Map"
        local = f.get("local_path", "")
        size = ""
        if local and Path(local).exists():
            size = f" ({Path(local).stat().st_size:,} bytes)"
        lines.append(
            f"- `{f.get('url', '')}`\n"
            f"  æœ¬åœ°: `{local}`{size}\n"
            f"  {sm} | åŸŸå: {f.get('domain', '')} | æ—¶é—´: {f.get('timestamp', '')}"
        )
    return "\n".join(lines)


@mcp.tool
async def read_js_file(
    file_path: Optional[str] = None,
    url: Optional[str] = None,
    start_line: int = 1,
    end_line: Optional[int] = None,
) -> str:
    """è¯»å–å·²æŠ“å–çš„ JS æ–‡ä»¶æºç ã€‚æ”¯æŒé€šè¿‡æœ¬åœ°è·¯å¾„æˆ– URL å®šä½æ–‡ä»¶ã€‚
    å¯æŒ‡å®šè¡ŒèŒƒå›´åªè¯»å–éƒ¨åˆ†ä»£ç ï¼Œé€‚åˆæŸ¥çœ‹å¤§æ–‡ä»¶çš„ç‰¹å®šåŒºåŸŸã€‚

    ä½¿ç”¨æµç¨‹: å…ˆç”¨ list_captured_files æŸ¥çœ‹æ–‡ä»¶åˆ—è¡¨ï¼Œå†ç”¨æœ¬å·¥å…·è¯»å–å…·ä½“æ–‡ä»¶ã€‚

    Args:
        file_path: JS æ–‡ä»¶çš„æœ¬åœ°ç»å¯¹è·¯å¾„ï¼ˆä»Ž list_captured_files èŽ·å–ï¼‰
        url: JS æ–‡ä»¶çš„åŽŸå§‹ URLï¼ˆäºŒé€‰ä¸€ï¼Œä¼˜å…ˆä½¿ç”¨ file_pathï¼‰
        start_line: èµ·å§‹è¡Œå·ï¼ˆä»Ž 1 å¼€å§‹ï¼Œé»˜è®¤ 1ï¼‰
        end_line: ç»“æŸè¡Œå·ï¼ˆä¸å¡«åˆ™è¯»åˆ°æ–‡ä»¶æœ«å°¾ï¼‰
    """
    target_path: Optional[Path] = None

    if file_path:
        target_path = Path(file_path)
    elif url:
        record = pipeline.index.get_file_by_url(url)
        if record and record.get("local_path"):
            target_path = Path(record["local_path"])
        else:
            return f"âŒ æœªæ‰¾åˆ° URL å¯¹åº”çš„æœ¬åœ°æ–‡ä»¶: {url}\nè¯·å…ˆä½¿ç”¨ capture_current_page æŠ“å–ã€‚"
    else:
        return "âŒ è¯·æä¾› file_path æˆ– url å‚æ•°ã€‚"

    if not target_path.exists():
        return f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {target_path}"

    try:
        content = target_path.read_text(encoding="utf-8", errors="replace")
    except Exception as e:
        return f"âŒ è¯»å–å¤±è´¥: {e}"

    all_lines = content.split("\n")
    total = len(all_lines)

    start = max(1, start_line) - 1
    end = min(total, end_line) if end_line else total
    selected = all_lines[start:end]

    header = f"ðŸ“„ `{target_path.name}` (è¡Œ {start + 1}-{end}/{total})\n"
    numbered = "\n".join(
        f"{start + 1 + i:>6} | {line}" for i, line in enumerate(selected)
    )
    return header + f"```javascript\n{numbered}\n```"


@mcp.tool
async def execute_js(expression: str, target_url: Optional[str] = None) -> str:
    """åœ¨å½“å‰æµè§ˆå™¨é¡µé¢ä¸Šä¸‹æ–‡ä¸­æ‰§è¡Œ JavaScript è¡¨è¾¾å¼å¹¶è¿”å›žç»“æžœã€‚
    ç”¨äºŽé€†å‘åˆ†æžæ—¶éªŒè¯å‡è®¾ã€è°ƒç”¨é¡µé¢å‡½æ•°ã€æ£€æŸ¥å˜é‡å€¼ç­‰ã€‚

    æ”¯æŒ await å¼‚æ­¥è¡¨è¾¾å¼ã€‚è¿”å›žå€¼ä¼šè‡ªåŠ¨ JSON åºåˆ—åŒ–ã€‚

    Args:
        expression: è¦æ‰§è¡Œçš„ JS è¡¨è¾¾å¼ï¼Œä¾‹å¦‚ "document.cookie" æˆ– "JSON.stringify(window._config)"
        target_url: ç›®æ ‡é¡µé¢ URLï¼ˆå¯é€‰ï¼Œä¸å¡«åˆ™ä½¿ç”¨å½“å‰è¿žæŽ¥çš„é¡µé¢ï¼‰
    """
    try:
        await pipeline._browser.ensure_connected(target_url=target_url)
        result = await pipeline._browser.evaluate(expression)
    except Exception as e:
        return f"âŒ æ‰§è¡Œå¤±è´¥: {e}"

    if result is None:
        return "æ‰§è¡Œå®Œæˆï¼Œè¿”å›žå€¼: undefined"

    if isinstance(result, str):
        return f"```\n{result}\n```"

    try:
        formatted = json.dumps(result, ensure_ascii=False, indent=2)
        return f"```json\n{formatted}\n```"
    except (TypeError, ValueError):
        return f"```\n{result}\n```"


@mcp.tool
async def capture_network_requests(
    target_url: Optional[str] = None,
    duration: float = 10.0,
    trigger_action: Optional[str] = None,
    filter_type: Optional[str] = None,
) -> str:
    """ç›‘å¬æµè§ˆå™¨ç½‘ç»œè¯·æ±‚ï¼Œæ•èŽ·æŒ‡å®šæ—¶é—´æ®µå†…çš„æ‰€æœ‰ XHR/Fetch/è„šæœ¬è¯·æ±‚ã€‚
    è¿™æ˜¯é€†å‘åˆ†æžçš„æ ¸å¿ƒå·¥å…·â€”â€”è§‚å¯Ÿé¡µé¢å‘å‡ºäº†å“ªäº› API è¯·æ±‚ã€æºå¸¦äº†ä»€ä¹ˆå‚æ•°å’Œç­¾åã€‚

    ä½¿ç”¨ trigger_action å‚æ•°ä¼ å…¥ä¸€æ®µ JS ä»£ç ï¼Œå·¥å…·ä¼šåœ¨å¼€å§‹ç›‘å¬åŽè‡ªåŠ¨æ‰§è¡Œå®ƒæ¥è§¦å‘ç½‘ç»œè¯·æ±‚ã€‚
    ä¾‹å¦‚ä¼ å…¥ "document.querySelector('#submit').click()" æ¥ç‚¹å‡»æäº¤æŒ‰é’®ã€‚
    å¦‚æžœä¸ä¼  trigger_actionï¼Œå·¥å…·ä¼šè‡ªåŠ¨åˆ·æ–°é¡µé¢æ¥è§¦å‘è¯·æ±‚ã€‚

    Args:
        target_url: ç›®æ ‡é¡µé¢ URLï¼ˆå¯é€‰ï¼Œä¸å¡«åˆ™ä½¿ç”¨å½“å‰é¡µé¢ï¼‰
        duration: ç›‘å¬æ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ 10 ç§’
        trigger_action: ç›‘å¬å¼€å§‹åŽè‡ªåŠ¨æ‰§è¡Œçš„ JS ä»£ç ï¼Œç”¨äºŽè§¦å‘ç½‘ç»œè¯·æ±‚ã€‚ä¾‹å¦‚ç‚¹å‡»æŒ‰é’®ã€æäº¤è¡¨å•ã€è°ƒç”¨ fetch ç­‰
        filter_type: è¿‡æ»¤è¯·æ±‚ç±»åž‹ï¼Œå¯é€‰ "XHR"ã€"Fetch"ã€"Script"ã€‚ä¸å¡«åˆ™æ•èŽ·æ‰€æœ‰ç±»åž‹
    """
    try:
        await pipeline._browser.ensure_connected(target_url=target_url)

        async def _trigger():
            await asyncio.sleep(0.3)
            if trigger_action:
                try:
                    await pipeline._browser.evaluate(trigger_action)
                except Exception as e:
                    logger.warning("trigger_action æ‰§è¡Œå¤±è´¥: %s", e)
            else:
                try:
                    await pipeline._browser.evaluate("location.reload()")
                except Exception:
                    pass

        asyncio.create_task(_trigger())
        events = await pipeline._browser.collect_network_events(duration_sec=duration)
    except Exception as e:
        return f"âŒ ç½‘ç»œç›‘å¬å¤±è´¥: {e}"

    if filter_type:
        events = [e for e in events if e.get("type", "").lower() == filter_type.lower()]

    if not events:
        return f"åœ¨ {duration} ç§’å†…æœªæ•èŽ·åˆ°ç½‘ç»œè¯·æ±‚ã€‚å°è¯•åœ¨é¡µé¢ä¸Šè§¦å‘æ“ä½œåŽé‡æ–°ç›‘å¬ã€‚"

    lines = [f"ðŸŒ æ•èŽ·åˆ° {len(events)} ä¸ªç½‘ç»œè¯·æ±‚ ({duration}s)\n"]
    for i, evt in enumerate(events, 1):
        resp = evt.get("response") or {}
        status = resp.get("status", "-")
        lines.append(
            f"### è¯·æ±‚ {i}\n"
            f"- **{evt.get('method', '?')}** `{evt.get('url', '')}`\n"
            f"- ç±»åž‹: {evt.get('type', '?')} | çŠ¶æ€: {status}\n"
            f"- å‘èµ·æ–¹: {evt.get('initiator', '?')}"
        )
        if evt.get("postData"):
            post = evt["postData"]
            if len(post) > 2000:
                post = post[:2000] + "...(æˆªæ–­)"
            lines.append(f"- POST æ•°æ®:\n```\n{post}\n```")

        req_headers = evt.get("headers", {})
        interesting = {
            k: v
            for k, v in req_headers.items()
            if k.lower()
            in (
                "authorization",
                "cookie",
                "x-token",
                "x-sign",
                "x-signature",
                "x-timestamp",
                "x-nonce",
                "content-type",
                "referer",
                "origin",
            )
        }
        if interesting:
            lines.append("- å…³é”®è¯·æ±‚å¤´:")
            for k, v in interesting.items():
                lines.append(f"  - `{k}`: `{v}`")

    return "\n".join(lines)


@mcp.tool
async def hook_function(
    function_path: str,
    target_url: Optional[str] = None,
    trigger_action: Optional[str] = None,
    max_calls: int = 10,
    duration: float = 15.0,
) -> str:
    """åœ¨é¡µé¢ä¸­ Hook æŒ‡å®šçš„ JavaScript å‡½æ•°ï¼Œè®°å½•å…¶è°ƒç”¨å‚æ•°ã€è¿”å›žå€¼å’Œè°ƒç”¨æ ˆã€‚
    è¿™æ˜¯é€†å‘åˆ†æžåŠ å¯†å‡½æ•°çš„å…³é”®å·¥å…·â€”â€”æ‰¾åˆ°å¯ç–‘å‡½æ•°åŽï¼Œç”¨ hook è§‚å¯Ÿå®žé™…çš„è¾“å…¥è¾“å‡ºã€‚

    ä½¿ç”¨ trigger_action å‚æ•°ä¼ å…¥ä¸€æ®µ JS ä»£ç ï¼Œå·¥å…·ä¼šåœ¨ Hook æ³¨å…¥åŽè‡ªåŠ¨æ‰§è¡Œå®ƒæ¥è§¦å‘å‡½æ•°è°ƒç”¨ã€‚
    ä¾‹å¦‚ä¼ å…¥ "document.querySelector('#login-btn').click()" æ¥è§¦å‘ç™»å½•æµç¨‹ã€‚
    å¦‚æžœä¸ä¼  trigger_actionï¼Œå·¥å…·ä¼šç­‰å¾… duration ç§’ï¼ŒæœŸé—´é¡µé¢è‡ªèº«çš„æ“ä½œå¯èƒ½è§¦å‘å‡½æ•°è°ƒç”¨ã€‚

    Args:
        function_path: è¦ hook çš„å‡½æ•°è·¯å¾„ï¼Œä¾‹å¦‚ "window.encrypt"ã€"JSON.stringify"ã€"CryptoJS.MD5"
        target_url: ç›®æ ‡é¡µé¢ URLï¼ˆå¯é€‰ï¼‰
        trigger_action: Hook æ³¨å…¥åŽè‡ªåŠ¨æ‰§è¡Œçš„ JS ä»£ç ï¼Œç”¨äºŽè§¦å‘ç›®æ ‡å‡½æ•°è°ƒç”¨
        max_calls: æœ€å¤šè®°å½•å¤šå°‘æ¬¡è°ƒç”¨ï¼Œé»˜è®¤ 10
        duration: ç›‘å¬æ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ 15 ç§’
    """
    safe_path = function_path.replace("'", "\\'")
    hook_js = (
        """
    (function() {
        var _hookedCalls = [];
        var _maxCalls = """
        + str(max_calls)
        + """;
        var _target;
        try { _target = """
        + function_path
        + """; } catch(e) {
            return JSON.stringify({error: '"""
        + safe_path
        + """ ä¸å­˜åœ¨: ' + e.message});
        }
        if (typeof _target !== 'function') {
            return JSON.stringify({error: '"""
        + safe_path
        + """ ä¸æ˜¯å‡½æ•°'});
        }
        var _original = _target;
        var _parts = '"""
        + safe_path
        + """'.split('.');
        var _parent = _parts.length > 1
            ? _parts.slice(0, -1).reduce(function(o, k) { return o[k]; }, window)
            : window;
        var _key = _parts[_parts.length - 1];

        _parent[_key] = function() {
            var args = Array.prototype.slice.call(arguments);
            var callInfo = {
                args: args.map(function(a) {
                    try { return JSON.stringify(a).substring(0, 500); }
                    catch(e) { return String(a).substring(0, 500); }
                }),
                stack: new Error().stack.split('\\n').slice(1, 6).map(function(s) { return s.trim(); }),
            };
            var result = _original.apply(this, arguments);
            try { callInfo.returnValue = JSON.stringify(result).substring(0, 500); }
            catch(e) { callInfo.returnValue = String(result).substring(0, 500); }
            if (_hookedCalls.length < _maxCalls) _hookedCalls.push(callInfo);
            return result;
        };

        window.__browserInsightHook = {
            calls: _hookedCalls,
            restore: function() { _parent[_key] = _original; },
        };
        return JSON.stringify({status: 'hooked', target: '"""
        + safe_path
        + """'});
    })()
    """
    )

    try:
        await pipeline._browser.ensure_connected(target_url=target_url)
        hook_result = await pipeline._browser.evaluate(hook_js)
    except Exception as e:
        return f"âŒ Hook å¤±è´¥: {e}"

    parsed = json.loads(hook_result) if isinstance(hook_result, str) else hook_result
    if isinstance(parsed, dict) and parsed.get("error"):
        return f"âŒ {parsed['error']}"

    if trigger_action:
        try:
            await pipeline._browser.evaluate(trigger_action)
        except Exception as e:
            logger.warning("trigger_action æ‰§è¡Œå¤±è´¥: %s", e)

    await asyncio.sleep(duration)

    try:
        calls_raw = await pipeline._browser.evaluate(
            "JSON.stringify(window.__browserInsightHook ? window.__browserInsightHook.calls : [])"
        )
        await pipeline._browser.evaluate(
            "window.__browserInsightHook && window.__browserInsightHook.restore()"
        )
    except Exception as e:
        return f"âŒ èŽ·å– Hook ç»“æžœå¤±è´¥: {e}"

    calls = json.loads(calls_raw) if isinstance(calls_raw, str) else calls_raw
    if not calls:
        return (
            f"åœ¨ {duration} ç§’å†… `{function_path}` æœªè¢«è°ƒç”¨ã€‚å°è¯•åœ¨é¡µé¢ä¸Šè§¦å‘ç›¸å…³æ“ä½œã€‚"
        )

    lines = [f"ðŸª `{function_path}` è¢«è°ƒç”¨ {len(calls)} æ¬¡ ({duration}s)\n"]
    for i, call in enumerate(calls, 1):
        lines.append(f"### è°ƒç”¨ {i}")
        lines.append(f"- å‚æ•°: {', '.join(call.get('args', []))}")
        lines.append(f"- è¿”å›žå€¼: {call.get('returnValue', 'undefined')}")
        stack = call.get("stack", [])
        if stack:
            lines.append("- è°ƒç”¨æ ˆ:")
            for s in stack[:5]:
                lines.append(f"  - `{s}`")

    return "\n".join(lines)


ENCRYPTION_PATTERNS = {
    "MD5": r"(?i)\b(md5|MD5|hex_md5)\s*\(",
    "SHA": r"(?i)\b(sha1|sha256|sha512|SHA)\s*\(",
    "AES": r"(?i)\b(AES|aes)\s*\.\s*(encrypt|decrypt|Encrypt|Decrypt)",
    "DES/3DES": r"(?i)\b(DES|TripleDES|des)\s*\.\s*(encrypt|decrypt)",
    "RSA": r"(?i)\b(RSA|rsa)\s*\.\s*(encrypt|decrypt|sign|verify)",
    "Base64": r"(?i)\b(btoa|atob|Base64|base64)\s*\(",
    "HMAC": r"(?i)\b(hmac|HMAC|HmacSHA|HmacMD5)\s*\(",
    "CryptoJS": r"CryptoJS\.\w+",
    "JSEncrypt": r"JSEncrypt|jsencrypt",
    "sign/signature": r"(?i)\b(sign|signature|getSign|makeSign|calcSign)\s*\(",
    "token/encrypt": r"(?i)\b(encrypt|decrypt|encode|decode|encryptData|decryptData)\s*\(",
}


@mcp.tool
async def analyze_encryption(domain_filter: Optional[str] = None) -> str:
    """æ‰«æå·²ç´¢å¼•çš„ä»£ç åº“ï¼Œè‡ªåŠ¨è¯†åˆ«å¸¸è§çš„åŠ å¯†/ç­¾åæ¨¡å¼ã€‚
    æ£€æµ‹ MD5ã€SHAã€AESã€RSAã€DESã€Base64ã€HMACã€è‡ªå®šä¹‰ç­¾åå‡½æ•°ç­‰ã€‚
    è¿”å›žåŒ¹é…çš„ä»£ç ç‰‡æ®µåŠå…¶ä½ç½®ï¼Œå¸®åŠ©å¿«é€Ÿå®šä½åŠ å¯†é€»è¾‘ã€‚

    Args:
        domain_filter: é™åˆ¶æ‰«æçš„åŸŸåï¼Œä¾‹å¦‚ "www.example.com"
    """
    all_matches: dict[str, list[dict]] = {}

    for name, pattern in ENCRYPTION_PATTERNS.items():
        matches = pipeline.index.search_chunks_by_text(
            pattern, domain=domain_filter, limit=20
        )
        if matches:
            filtered = []
            for m in matches:
                text = m.get("text", "")
                found = re.findall(pattern, text)
                if found:
                    filtered.append(m)
            if filtered:
                all_matches[name] = filtered

    if not all_matches:
        return (
            "æœªæ£€æµ‹åˆ°å¸¸è§åŠ å¯†æ¨¡å¼ã€‚å¯èƒ½ä½¿ç”¨äº†è‡ªå®šä¹‰æ··æ·†æˆ– WASM åŠ å¯†ã€‚\n"
            "å»ºè®®ä½¿ç”¨ capture_network_requests è§‚å¯Ÿ API è¯·æ±‚ä¸­çš„åŠ å¯†å‚æ•°ã€‚"
        )

    lines = ["ðŸ” åŠ å¯†æ¨¡å¼åˆ†æžç»“æžœ\n"]
    total = sum(len(v) for v in all_matches.values())
    lines.append(f"å…±æ£€æµ‹åˆ° {total} å¤„åŠ å¯†ç›¸å…³ä»£ç ï¼Œæ¶‰åŠ {len(all_matches)} ç§æ¨¡å¼:\n")

    for name, matches in all_matches.items():
        lines.append(f"## {name} ({len(matches)} å¤„)")
        for m in matches[:5]:
            text = m.get("text", "")
            if len(text) > 500:
                text = text[:500] + "..."
            lines.append(
                f"- æ–‡ä»¶: `{m.get('original_file', '?')}` "
                f"(è¡Œ {m.get('line_start', '?')}-{m.get('line_end', '?')})\n"
                f"```javascript\n{text}\n```"
            )
        if len(matches) > 5:
            lines.append(f"  ...è¿˜æœ‰ {len(matches) - 5} å¤„")

    lines.append(
        "\nðŸ’¡ å»ºè®®ä¸‹ä¸€æ­¥:\n"
        "1. ç”¨ read_js_file æŸ¥çœ‹å®Œæ•´çš„åŠ å¯†å‡½æ•°\n"
        "2. ç”¨ hook_function è§‚å¯ŸåŠ å¯†å‡½æ•°çš„å®žé™…è¾“å…¥è¾“å‡º\n"
        "3. ç”¨ execute_js åœ¨é¡µé¢ä¸­è°ƒç”¨åŠ å¯†å‡½æ•°éªŒè¯"
    )
    return "\n".join(lines)


@mcp.resource("insight://archived-sites")
def list_archived_sites() -> str:
    """åˆ—å‡ºæœ¬åœ°å·²å½’æ¡£çš„æ‰€æœ‰åŸŸåå’ŒæŠ“å–è®°å½•ã€‚"""
    domains = pipeline.index.list_domains()
    if not domains:
        return "æœ¬åœ°æš‚æ— å½’æ¡£æ•°æ®ã€‚è¯·å…ˆä½¿ç”¨ capture_current_page æŠ“å–é¡µé¢ã€‚"

    total_files = pipeline.index.get_file_count()
    total_chunks = pipeline.index.get_chunk_count()

    lines = [f"ðŸ“Š æœ¬åœ°å½’æ¡£æ¦‚è§ˆ (å…± {total_files} ä¸ªæ–‡ä»¶, {total_chunks} ä¸ªä»£ç å—)\n"]
    for d in domains:
        lines.append(
            f"- **{d['domain']}**: {d['file_count']} ä¸ªæ–‡ä»¶, æœ€è¿‘æŠ“å–: {d['latest']}"
        )
    return "\n".join(lines)


def main() -> None:
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(name)s] %(levelname)s: %(message)s",
    )
    mcp.run()


if __name__ == "__main__":
    main()
