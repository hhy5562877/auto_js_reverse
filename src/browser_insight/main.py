from __future__ import annotations

import json
import logging
from pathlib import Path
from typing import Optional

from fastmcp import FastMCP

from .services.pipeline import Pipeline

logger = logging.getLogger(__name__)

BASE_DIR = Path(__file__).resolve().parent.parent.parent
CONFIG_PATH = BASE_DIR / ".mcp_config" / "config.json"


def _load_config() -> dict:
    if CONFIG_PATH.exists():
        return json.loads(CONFIG_PATH.read_text(encoding="utf-8"))
    return {}


config = _load_config()
pipeline = Pipeline(config=config, base_dir=BASE_DIR)

mcp = FastMCP(name="Browser Insight MCP")


@mcp.tool
async def capture_current_page(
    storage_path: str, target_url: Optional[str] = None, force_refresh: bool = False
) -> str:
    """è§¦å‘ä¸€æ¬¡å®Œæ•´çš„å…¨é‡æŠ“å–ã€å½’æ¡£ã€åˆ†æžæµç¨‹ã€‚
    æŠ“å– Chrome é¡µé¢çš„æ‰€æœ‰ JS èµ„æºï¼Œè¿›è¡Œ Source Map è¿˜åŽŸã€AST è¯­ä¹‰åˆ‡åˆ†ã€å‘é‡åŒ–ç´¢å¼•ã€‚
    æ‰€æœ‰åŽŸå§‹ JS æ–‡ä»¶ã€Source Map å’Œè¿˜åŽŸåŽçš„æºç éƒ½ä¼šä¿å­˜åˆ°æŒ‡å®šçš„å­˜å‚¨è·¯å¾„ä¸‹ã€‚

    å¦‚æžœç”¨æˆ·å·²æœ‰æµè§ˆå™¨æ‰“å¼€äº†ç›®æ ‡ç½‘é¡µï¼Œä¼šè‡ªåŠ¨å¤ç”¨è¯¥æ ‡ç­¾é¡µï¼›å¦‚æžœæ²¡æœ‰åˆ™æ–°å»ºæ ‡ç­¾é¡µå¹¶å¯¼èˆªã€‚
    ä¸æŒ‡å®š target_url æ—¶ï¼Œä½¿ç”¨å½“å‰æ´»è·ƒæ ‡ç­¾é¡µã€‚

    Args:
        storage_path: æ–‡ä»¶å­˜å‚¨çš„ç»å¯¹è·¯å¾„ï¼Œæ‰€æœ‰æŠ“å–çš„ JS æ–‡ä»¶å°†å½’æ¡£åˆ°æ­¤ç›®å½•ä¸‹ï¼ˆæŒ‰æ—¥æœŸ/åŸŸå/åŽŸå§‹è·¯å¾„ç»„ç»‡ï¼‰
        target_url: ç›®æ ‡ç½‘é¡µ URLï¼Œä¾‹å¦‚ "https://www.baidu.com"ã€‚ä¼šè‡ªåŠ¨æŸ¥æ‰¾å·²æ‰“å¼€çš„åŒ¹é…æ ‡ç­¾é¡µï¼Œæ‰¾ä¸åˆ°åˆ™æ–°å»º
        force_refresh: æ˜¯å¦å¿½ç•¥å“ˆå¸Œç¼“å­˜ï¼Œå¼ºåˆ¶é‡æ–°è§£æžæ‰€æœ‰æ–‡ä»¶
    """
    try:
        stats = await pipeline.capture_page(
            force_refresh=force_refresh,
            storage_path=storage_path,
            target_url=target_url,
        )
    except ConnectionRefusedError as e:
        return f"âŒ è¿žæŽ¥å¤±è´¥: {e}"
    except Exception as e:
        logger.exception("capture_current_page å¼‚å¸¸")
        return f"âŒ æŠ“å–å¤±è´¥: {e}"

    parts = []
    parts.append(f"âœ… æŠ“å–å®Œæˆ")
    parts.append(f"æ–°å¢ž {stats['new_files']} ä¸ª JS æ–‡ä»¶")
    if stats["skipped"] > 0:
        parts.append(f"è·³è¿‡ {stats['skipped']} ä¸ªå·²ç´¢å¼•æ–‡ä»¶")
    if stats["source_maps"] > 0:
        parts.append(f"è¿˜åŽŸäº† {stats['source_maps']} ä¸ª Source Map")
    parts.append(f"å…±ç´¢å¼• {stats['chunks_indexed']} ä¸ªä»£ç å—")
    parts.append(f"å­˜å‚¨è·¯å¾„: {stats['storage_path']}")
    return "ï¼Œ".join(parts) + "ã€‚"


@mcp.tool
async def search_local_codebase(
    query: str, domain_filter: Optional[str] = None, limit: int = 10
) -> str:
    """RAG æ£€ç´¢æœ¬åœ°å·²ç´¢å¼•çš„æµè§ˆå™¨ JS ä»£ç åº“ã€‚

    Args:
        query: è‡ªç„¶è¯­è¨€æœç´¢é—®é¢˜ï¼Œä¾‹å¦‚ "ç”¨æˆ·ç™»å½•é€»è¾‘" æˆ– "API è¯·æ±‚å°è£…"
        domain_filter: é™åˆ¶æœç´¢çš„åŸŸåï¼Œä¾‹å¦‚ "example.com"
        limit: è¿”å›žç»“æžœæ•°é‡ä¸Šé™
    """
    try:
        results = await pipeline.search(
            query=query, domain_filter=domain_filter, limit=limit
        )
    except Exception as e:
        logger.exception("search_local_codebase å¼‚å¸¸")
        return f"âŒ æœç´¢å¤±è´¥: {e}"

    if not results:
        return "æœªæ‰¾åˆ°ç›¸å…³ä»£ç ã€‚è¯·å…ˆä½¿ç”¨ capture_current_page æŠ“å–é¡µé¢ã€‚"

    output_parts = []
    for i, r in enumerate(results, 1):
        source_tag = (
            "ðŸ”„ Source Map è¿˜åŽŸ" if r.get("source_map_restored") else "ðŸ“¦ æ··æ·†ä»£ç "
        )
        header = (
            f"### ç»“æžœ {i} [{source_tag}]\n"
            f"- æ–‡ä»¶: `{r.get('original_file', 'unknown')}`\n"
            f"- æ¥æº: `{r.get('url', '')}`\n"
            f"- è¡Œå·: {r.get('line_start', '?')}-{r.get('line_end', '?')}\n"
        )
        code = f"```javascript\n{r.get('text', '')}\n```"
        output_parts.append(header + code)

    return "\n\n".join(output_parts)


@mcp.resource("insight://archived-sites")
def list_archived_sites() -> str:
    """åˆ—å‡ºæœ¬åœ°å·²å½’æ¡£çš„æ‰€æœ‰åŸŸåå’ŒæŠ“å–è®°å½•ã€‚"""
    domains = pipeline.index.list_domains()
    if not domains:
        return "æœ¬åœ°æš‚æ— å½’æ¡£æ•°æ®ã€‚è¯·å…ˆä½¿ç”¨ capture_current_page æŠ“å–é¡µé¢ã€‚"

    total_files = pipeline.index.get_file_count()
    total_chunks = pipeline.index.get_chunk_count()

    lines = [f"ðŸ“Š æœ¬åœ°å½’æ¡£æ¦‚è§ˆ (å…± {total_files} ä¸ªæ–‡ä»¶, {total_chunks} ä¸ªä»£ç å—)\n"]
    for d in domains:
        lines.append(
            f"- **{d['domain']}**: {d['file_count']} ä¸ªæ–‡ä»¶, æœ€è¿‘æŠ“å–: {d['latest']}"
        )
    return "\n".join(lines)


def main() -> None:
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(name)s] %(levelname)s: %(message)s",
    )
    mcp.run()


if __name__ == "__main__":
    main()
